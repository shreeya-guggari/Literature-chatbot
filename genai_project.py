# -*- coding: utf-8 -*-
"""genai project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bte5E1DUZZA4ce1TbhRw0Kg_3KTSlpvo

Phase 1
"""

pip install -U bitsandbytes

!pip install transformers==4.36.2
!pip install datasets
!pip install accelerate
!|pip install peft

!pip install pandas numpy spacy matplotlib seaborn scikit-learn transformers
!python -m spacy download en_core_web_sm

pip install transformers peft accelerate datasets

pip install langchain faiss-cpu transformers accelerate sentence-transformers pandas

!pip install openai faiss-cpu pandas scikit-learn sentence-transformers

pip install -U google-generativeai

pip install networkx matplotlib

import pandas as pd
import spacy
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.manifold import TSNE
from transformers import BertTokenizer, BertModel
import torch

# Load dataset
df = pd.read_csv("/content/books.csv")  # Replace with your CSV file path
df.dropna(subset=["Description"], inplace=True)

# Initialize spaCy
nlp = spacy.load("en_core_web_sm")

# --------- 1. Preprocessing ---------
def preprocess(text):
    doc = nlp(text.lower())
    tokens = [
        token.lemma_ for token in doc
        if not token.is_stop and not token.is_punct and token.is_alpha
    ]
    return tokens

df["Tokens"] = df["Description"].apply(preprocess)

# --------- 2. POS Tagging ---------
def get_pos_distribution(text):
    doc = nlp(text)
    pos_counts = {}
    for token in doc:
        if token.pos_ in pos_counts:
            pos_counts[token.pos_] += 1
        else:
            pos_counts[token.pos_] = 1
    return pos_counts

df["POS_Distribution"] = df["Description"].apply(get_pos_distribution)

# Optional: Aggregate POS tags across the dataset
from collections import Counter
pos_agg = Counter()
df["POS_Distribution"].apply(lambda x: pos_agg.update(x))
print("Overall POS Distribution:")
print(dict(pos_agg))

# --------- 3. BERT Embedding ---------
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
model.eval()

def get_bert_embedding(text):
    with torch.no_grad():
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=64)
        outputs = model(**inputs)
        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token
        return cls_embedding.squeeze().numpy()

# Get embeddings (limit to first N descriptions for speed)
N = 200  # Limit for t-SNE visualization
sample_df = df.head(N).copy()
sample_df["BERT_Embedding"] = sample_df["Description"].apply(get_bert_embedding)

# --------- 4. Visualize Embeddings ---------
embeddings = np.stack(sample_df["BERT_Embedding"].values)
tsne = TSNE(n_components=2, random_state=42, perplexity=30)
reduced = tsne.fit_transform(embeddings)

# Plot
plt.figure(figsize=(10, 7))
sns.scatterplot(x=reduced[:, 0], y=reduced[:, 1], hue=sample_df["Genres"], palette="Set2", legend=False)
plt.title("t-SNE Visualization of BERT Embeddings (Descriptions)")
plt.xlabel("TSNE-1")
plt.ylabel("TSNE-2")
plt.show()

"""Phase 2"""

def chain_of_thought(query):
    print("üß† Chain of Thought Reasoning:")
    print("Step 1: Parse and understand the user query.")
    print("‚Üí Extract keywords: genres/themes + quality filter")

    keywords = ['dystopian', 'time travel', 'war', 'romance', 'future']
    selected = [k for k in keywords if k in query.lower()]
    print(f"Step 2: Matched Keywords: {selected}")

    print("Step 3: Filter books that match description and genres.")
    result = df[df['Description'].str.contains('|'.join(selected), case=False)]

    print("Step 4: Sort by rating and number of ratings.")
    result = result.sort_values(by=['Avg_Rating', 'Num_Ratings'], ascending=False)

    print("Step 5: Recommend top result.")
    return result[['Book', 'Author', 'Genres', 'Avg_Rating']].head(3)

# Example usage
chain_of_thought("Suggest a book that explores dystopian themes and has high reader ratings")

def tree_of_thought(genre=None, mood=None, min_rating=4.0):
    print("üå≤ Tree of Thought Reasoning:")

    print("Step 1: Branch on Genre")
    branch_genre = df[df['Genres'].str.contains(genre.upper())] if genre else df

    print(f"Step 2: Branch on Mood = {mood}")
    if mood:
        branch_genre = branch_genre[branch_genre['Description'].str.contains(mood.lower())]

    print("Step 3: Branch on Rating Threshold")
    branch_filtered = branch_genre[branch_genre['Avg_Rating'] >= min_rating]

    print("Step 4: Return top 3 books")
    return branch_filtered[['Book', 'Author', 'Genres', 'Avg_Rating']].head(3)

# Example usage:
tree_of_thought(genre="Science Fiction", mood="adventure", min_rating=4.3)

import networkx as nx
import matplotlib.pyplot as plt

def graph_of_thought():
    print("üîó Graph of Thought Reasoning:")
    G = nx.Graph()

    # Build edges between genres in the same book
    for genres in df['Genres']:
        genre_list = [g.strip() for g in genres.split(',')]
        for i in range(len(genre_list)):
            for j in range(i + 1, len(genre_list)):
                G.add_edge(genre_list[i], genre_list[j])

    print("Step 1: Build graph from co-occurring genres.")
    print("Step 2: Visualize connections.")

    plt.figure(figsize=(12, 8))
    nx.draw_networkx(G, with_labels=True, node_color='lightgreen', edge_color='gray', font_weight='bold')
    plt.title("Graph of Thought: Genre Interconnections")
    plt.show()

# Run it
graph_of_thought()

!pip install plotly
import pandas as pd
import networkx as nx
import plotly.graph_objects as go

# Load your dataset
df = pd.read_csv("/content/books.csv")
df['Genres'] = df['Genres'].str.upper()
df = df[['Book', 'Genres']].dropna()

# Build the graph of genre relationships
G = nx.Graph()
for genres in df['Genres']:
    genre_list = [g.strip() for g in genres.split(',')]
    for i in range(len(genre_list)):
        for j in range(i + 1, len(genre_list)):
            if G.has_edge(genre_list[i], genre_list[j]):
                G[genre_list[i]][genre_list[j]]['weight'] += 1
            else:
                G.add_edge(genre_list[i], genre_list[j], weight=1)

# Filter edges with a minimum weight to reduce clutter
G_filtered = nx.Graph((u, v, d) for u, v, d in G.edges(data=True) if d["weight"] >= 5)

# Get positions
pos = nx.spring_layout(G_filtered, k=0.8, seed=42)

# Create edges
edge_x = []
edge_y = []
for edge in G_filtered.edges():
    x0, y0 = pos[edge[0]]
    x1, y1 = pos[edge[1]]
    edge_x += [x0, x1, None]
    edge_y += [y0, y1, None]

edge_trace = go.Scatter(
    x=edge_x, y=edge_y,
    line=dict(width=1, color='rgba(180,180,180,0.5)'),
    hoverinfo='none',
    mode='lines'
)

# Create nodes
node_x = []
node_y = []
labels = []
for node in G_filtered.nodes():
    x, y = pos[node]
    node_x.append(x)
    node_y.append(y)
    labels.append(node)

node_trace = go.Scatter(
    x=node_x, y=node_y,
    mode='markers+text',
    text=labels,
    textposition="top center",
    hoverinfo='text',
    marker=dict(
        showscale=False,
        color='lightblue',
        size=18,
        line=dict(width=2, color='darkblue')
    )
)

# Final Plot
fig = go.Figure(data=[edge_trace, node_trace],
         layout=go.Layout(
            title='<b>Graph of Thought: Genre Relationships</b>',
            titlefont_size=22,
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=50),
            plot_bgcolor='white',
            xaxis=dict(showgrid=False, zeroline=False, visible=False),
            yaxis=dict(showgrid=False, zeroline=False, visible=False)
        )
)

fig.show()

"""Phase 3"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# 1Ô∏è‚É£ Load book data
df = pd.read_csv("/content/books.csv")  # replace with your actual path

# 2Ô∏è‚É£ Create combined book text for embeddings
df["content"] = df.apply(
    lambda row: f"Title: {row['Book']}\nAuthor: {row['Author']}\nGenres: {row['Genres']}\nRating: {row['Avg_Rating']}\nDescription: {row['Description']}",
    axis=1
)

# 3Ô∏è‚É£ Generate embeddings with sentence-transformers
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
corpus_embeddings = embed_model.encode(df["content"].tolist(), show_progress_bar=True)

# 4Ô∏è‚É£ Build FAISS index for similarity search
dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(corpus_embeddings))

def retrieve_similar_books(query, k=5):
    query_embedding = embed_model.encode([query])
    _, indices = index.search(np.array(query_embedding), k)
    return df.iloc[indices[0]]["content"].tolist()

#Load TinyLLaMA model from Hugging Face
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# 6Ô∏è‚É£ RAG Generation Function
def generate_answer_with_context(user_query, context):
    context_text = "\n\n".join(context)

    # Dynamic prompt generation for different types of queries
    prompt = f"""You are a helpful book assistant. Based on the following books, answer the user's query.

User Query: {user_query}

Relevant Books:
{context_text}

Answer:"""

    # Generate the response with a controlled length for relevance
    result = text_generator(prompt, max_new_tokens=128, do_sample=True, temperature=0.7)
    return result[0]["generated_text"]

# 7Ô∏è‚É£ Refined context retrieval function
def retrieve_similar_books(user_query, k=5):
    # Check if the question is asking for all books by a specific author
    if "books by" in user_query.lower():
        author_name = user_query.lower().split("books by")[-1].strip()
        # Filter books by the author name (case insensitive)
        author_books = df[df['Author'].str.contains(author_name, case=False, na=False)]

        if author_books.empty:
            return ["Sorry, no books found for that author."]

        # Return a list of book titles for the given author
        return author_books['content'].tolist()

    # For other queries, retrieve books based on relevance
    else:
        # Retrieve similar books (using an improved similarity measure)
        query_embedding = embed_model.encode([user_query])
        _, indices = index.search(np.array(query_embedding), k)

        return df.iloc[indices[0]]['content'].tolist()

# 8Ô∏è‚É£ Final RAG pipeline with better handling of queries
def book_rag_recommender(user_question):
    context = retrieve_similar_books(user_question, k=5)
    return generate_answer_with_context(user_question, context)

# üöÄ Example usage
if __name__ == "__main__":
    question = "Which books were written by George Orwell?"
    answer = book_rag_recommender(question)
    print("\nüìö Recommendation:")
    print(answer)

    # Example for a different query
    question2 = "Tell me about a good science fiction book."
    answer2 = book_rag_recommender(question2)
    print("\nüìö Recommendation:")
    print(answer2)

"""Phase 5

"""

pip install peft accelerate datasets evaluate

!pip install -U langchain-community

!pip install -q transformers datasets peft accelerate bitsandbytes

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel
from torch.optim import AdamW # Import AdamW from torch.optim
from sklearn.model_selection import train_test_split
import pandas as pd

import pandas as pd

# Load your dataset
df = pd.read_csv("/content/books.csv")  # change path if needed
df = df.dropna(subset=["Description"])  # Drop rows without description

# Combine text fields if needed
df["text"] = df["Book"] + " by " + df["Author"] + ". " + df["Description"]

# Create labels (e.g., based on Genre or binning Avg_Rating)
df["label"] = df["Genres"].astype(str)  # You can encode this later

df = df[["text", "label"]]  # Keep only relevant columns
df.head()

!pip install datasets

from datasets import Dataset
from sklearn.preprocessing import LabelEncoder

# Encode labels
le = LabelEncoder()
df["label"] = le.fit_transform(df["label"])

dataset = Dataset.from_pandas(df)

# Split into train/test
dataset = dataset.train_test_split(test_size=0.2)

from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_id = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_id)

def tokenize(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized = dataset.map(tokenize)

num_labels = df["label"].nunique()
model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=num_labels)

from peft import get_peft_model, LoraConfig, TaskType

config = LoraConfig(
    r=8,                      # Rank
    lora_alpha=16,
    target_modules=["query", "value"],  # Adjust for model type
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS  # Sequence classification
)

model = get_peft_model(model, config)
model.print_trainable_parameters()

!pip install -U transformers



from transformers import TrainingArguments, Trainer

import os
os.environ["WANDB_DISABLED"] = "true"


args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    logging_dir="./logs",
    learning_rate=2e-4,
    # Remove evaluation_strategy
    save_steps=1000,  # Save model every 1000 steps
    logging_steps=100,  # Log every 100 steps
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    eval_dataset=tokenized["test"],
    tokenizer=tokenizer
)

trainer.train()

model.save_pretrained("./book-lora-model")
tokenizer.save_pretrained("./book-lora-model")

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the fine-tuned model and tokenizer
model = AutoModelForCausalLM.from_pretrained("./book-lora-model")
tokenizer = AutoTokenizer.from_pretrained("./book-lora-model")

# Example question
book_info = "Title: The Great Gatsby\nAuthor: F. Scott Fitzgerald\nGenres: Fiction\nRating: 4.2\nDescription: A story about the American Dream."
question = "Who is the author of this book?"

# Format the input as a prompt
input_text = f"{book_info}\n\nQuestion: {question}\nAnswer:"

# Tokenize the input
inputs = tokenizer(input_text, return_tensors="pt")

# Generate the answer
outputs = model.generate(inputs['input_ids'], max_length=100)

# Decode the output to get the generated text
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Extract the answer (after the "Answer:" part)
answer = generated_text.split("Answer:")[-1].strip()

print(f"Answer: {answer}")

"""Phase 4"""

!pip install openai-whisper
!pip install torch

!pip install pyttsx3

!pip install openai-whisper
!pip install pyttsx3

!apt-get update && apt-get install -y espeak espeak-ng

import whisper
import pyttsx3
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# 1Ô∏è‚É£ Load book data
df = pd.read_csv("/content/books.csv")  # replace with your actual path

# 2Ô∏è‚É£ Create combined book text for embeddings
df["content"] = df.apply(
    lambda row: f"Title: {row['Book']}\nAuthor: {row['Author']}\nGenres: {row['Genres']}\nRating: {row['Avg_Rating']}\nDescription: {row['Description']}",
    axis=1
)

# 3Ô∏è‚É£ Generate embeddings with sentence-transformers
embed_model = SentenceTransformer("all-MiniLM-L6-v2")
corpus_embeddings = embed_model.encode(df["content"].tolist(), show_progress_bar=True)

# 4Ô∏è‚É£ Build FAISS index for similarity search
dimension = corpus_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(corpus_embeddings))

# Function to retrieve relevant books based on a query
def retrieve_similar_books(query, k=5):
    query_embedding = embed_model.encode([query])
    _, indices = index.search(np.array(query_embedding), k)
    return df.iloc[indices[0]]["content"].tolist()

# 5Ô∏è‚É£ Load TinyLLaMA model from Hugging Face
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v0.3"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

text_generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# 6Ô∏è‚É£ RAG Generation Function
def generate_answer_with_context(user_query, context):
    context_text = "\n\n".join(context)
    prompt = f"""You are a helpful book assistant. Based on the following books, answer the user's query.

User Query: {user_query}

Relevant Books:
{context_text}

Answer:"""

    result = text_generator(prompt, max_length=512, do_sample=True, temperature=0.7)
    return result[0]["generated_text"]

def book_rag_recommender(user_question):
    # Step 1: Retrieve relevant books based on user query
    context = retrieve_similar_books(user_question, k=5)  # Retrieve k relevant books

    # Step 2: Generate answer with context
    response = generate_answer_with_context(user_question, context)

    # Step 3: Clean the response to only include relevant details and remove excess information
    relevant_books = []

    # For each book content in context, find the corresponding book details from the dataframe
    for book_content in context:
        # Find the book row in the dataframe based on the content
        book_info = df[df['content'] == book_content].iloc[0]
        relevant_books.append(f"**Title:** {book_info['Book']}\n**Author:** {book_info['Author']}\n**Rating:** {book_info['Avg_Rating']}\n**Description:** {book_info['Description']}\n")

    # Step 4: Return formatted string of all relevant book details
    return "\n".join(relevant_books)



# 7Ô∏è‚É£ Convert Audio to Text using Whisper
def convert_audio_to_text(audio_file_path):
    model = whisper.load_model("base")  # Load Whisper model
    result = model.transcribe(audio_file_path)  # Transcribe the audio
    print(f"Converted Text: {result['text']}")
    return result['text']

!apt-get update && apt-get install -y espeak espeak-ng
import whisper
import pyttsx3
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline





# 8Ô∏è‚É£ Text-to-Speech Function (Optional for response output)
def speak_answer(answer):
    engine = pyttsx3.init()
    engine.say(answer)
    engine.runAndWait()

# 9Ô∏è‚É£ Integrating Audio Input with RAG (using Whisper)
def handle_audio_input(audio_file_path):
    # Step 1: Convert Audio to Text using Whisper
    user_question = convert_audio_to_text(audio_file_path)

    if user_question:
        # Step 2: Use the RAG system with the converted text (user's question)
        answer = book_rag_recommender(user_question)

        # Step 3: Output the answer (either as text or speech)
        print("\nüìö Recommendation:")
        print(answer)

        # Optional: Speak the answer aloud
        speak_answer(answer)
    else:
        print("Sorry, I couldn't process the audio correctly.")

# üöÄ Example Usage
if __name__ == "__main__":
    audio_file_path = "/content/2.in-speech.wav"  # Specify the audio file path

    # Handle audio input and get recommendations
    handle_audio_input(audio_file_path)

